{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"out",
				"outp"
			],
			[
				"prin",
				"printer_vec"
			],
			[
				"p",
				"pii"
			],
			[
				"spars",
				"spare table struct"
			],
			[
				"sou",
				"sour"
			],
			[
				"ma",
				"matrix_power_final"
			],
			[
				"re",
				"rep"
			],
			[
				"j",
				"abhj Single Test"
			],
			[
				"de",
				"debug"
			],
			[
				"be",
				"beginend"
			],
			[
				"ta",
				"koita"
			],
			[
				"per",
				"pre_str"
			],
			[
				"pe",
				"pre_str"
			],
			[
				"begi",
				"beginend"
			],
			[
				"rd",
				"red_points"
			],
			[
				"r",
				"range_query"
			],
			[
				"a",
				"abhj Multiple Test"
			],
			[
				"cp",
				"cp duplicate"
			],
			[
				"rep",
				"repeaterIndex"
			],
			[
				"repe",
				"repeaterIndex"
			],
			[
				"s",
				"string"
			],
			[
				"ss",
				"wonderfulSubstrings"
			],
			[
				"int",
				"fprintf"
			],
			[
				"t",
				"string"
			],
			[
				"m",
				"main"
			],
			[
				"ke",
				"keep2"
			],
			[
				"rem",
				"remo2"
			],
			[
				"kee",
				"keep2"
			],
			[
				"c",
				"c"
			],
			[
				"e",
				"erase"
			],
			[
				"debu",
				"debug"
			],
			[
				"co",
				"cour"
			],
			[
				"abh",
				"abhj Multiple Test"
			],
			[
				"fo",
				"forv"
			],
			[
				"by_",
				"by_x"
			],
			[
				"ab",
				"abhj Multiple Test"
			],
			[
				"ve",
				"vector"
			],
			[
				"whi",
				"while"
			],
			[
				"for",
				"for_"
			],
			[
				"se",
				"segment-tree\tsimpleSegmentTree"
			],
			[
				"fi",
				"findLexicographicRank"
			],
			[
				"pr",
				"prime_vector_sieve"
			],
			[
				"emp",
				"emplace"
			],
			[
				"pre",
				"pos_pre"
			],
			[
				"pos",
				"pos_pre"
			],
			[
				"cnt",
				"cnt_no"
			],
			[
				"cnt_",
				"cnt_yes"
			],
			[
				"ri",
				"running_sum"
			],
			[
				"jay_bh",
				"jay_bholenath_jay_bholeneth_pok_pok_pok"
			],
			[
				"dis",
				"dis_from_ori"
			],
			[
				"di",
				"dis_from_ori"
			],
			[
				"siz",
				"sizeof"
			],
			[
				"empalc",
				"emplace_back"
			]
		]
	},
	"buffers":
	[
		{
			"contents": " ",
			"settings":
			{
				"buffer_size": 1,
				"line_ending": "Unix"
			},
			"undo_stack":
			[
				[
					3,
					1,
					"insert",
					{
						"characters": "fduiop"
					},
					"BgAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAQAAAAAAAAACAAAAAAAAAAAAAAACAAAAAAAAAAMAAAAAAAAAAAAAAAMAAAAAAAAABAAAAAAAAAAAAAAABAAAAAAAAAAFAAAAAAAAAAAAAAAFAAAAAAAAAAYAAAAAAAAAAAAAAA",
					"DQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/"
				],
				[
					4,
					1,
					"delete_word",
					{
						"forward": false
					},
					"AQAAAAAAAAAAAAAAAAAAAAAAAAAGAAAAZmR1aW9w",
					"DQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAABgAAAAAAAAAGAAAAAAAAAAAAAAAAAPC/"
				],
				[
					13,
					1,
					"sequence",
					{
						"commands":
						[
							[
								"insert",
								{
									"characters": "dd"
								}
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							],
							[
								"left_delete",
								null
							]
						]
					},
					"BAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAQAAAAAAAAACAAAAAAAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAGQAAAAAAAAAAAAAAAAAAAAAAQAAAGQ",
					"DQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/"
				],
				[
					2,
					1,
					"insert",
					{
						"characters": " "
					},
					"AQAAAAAAAAAAAAAAAQAAAAAAAAAAAAAA",
					"DAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvw"
				]
			]
		},
		{
			"file": "/home/abj/Downloads/NLP_CS60075_A21_Assn1 (1).ipynb",
			"settings":
			{
				"buffer_size": 22961,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "contest/d.cpp",
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "const int M    =     1e9 + 7;\nint add(int a, int b) {\n\ta += b;\n\treturn (a >= M ? a - M : a);\n}\nint mul(int a, int b) {\n\treturn (a * b) % M;\n}\nint sub(int a, int b) {\n\treturn (a - b + M) % M;\n}\nint powM(int b, int p) {\n\tint r = 1;\n\tfor (; p; b = mul(b, b), p >>= 1)\n\t\tif (p & 1)\n\t\t\tr = mul(r, b);\n\treturn r;\n}\nint invM(int x) {\n\treturn powM(x, M - 2);\n}\nint f[N], b[N];\nvoid binomialCoeff() {\n\tf[0] = 1;\n\tfor (int i = 1; i <= N - 1; i++)\n\t\tf[i] = mul(f[i - 1], i);\n\tb[N - 1] = powM(f[N - 1], M - 2);\n\tfor (int i = N - 2; i >= 0; i--)\n\t\tb[i] = mul(b[i + 1], i + 1);\n}\nint C(int n, int r) {\n\treturn (r > n ? 0LL : mul(f[n], mul(b[r], b[n - r])));\n}",
			"file": "contest/c.cpp",
			"file_size": 0,
			"file_write_time": 132758175249476087,
			"settings":
			{
				"buffer_size": 645,
				"line_ending": "Unix"
			},
			"undo_stack":
			[
				[
					13,
					1,
					"sequence",
					{
						"commands":
						[
							[
								"insert",
								{
									"characters": "modu"
								}
							],
							[
								"insert_completion",
								{
									"completion": "const int M    =     1e9 + 7;\nint add(int a, int b) {\n\ta += b;\n\treturn (a >= M ? a - M : a);\n}\nint mul(int a, int b) {\n\treturn (a * b) % M;\n}\nint sub(int a, int b) {\n\treturn (a - b + M) % M;\n}\nint powM(int b, int p) {\n\tint r = 1;\n\tfor (; p; b = mul(b, b), p >>= 1)\n\t\tif (p & 1)\n\t\t\tr = mul(r, b);\n\treturn r;\n}\nint invM(int x) {\n\treturn powM(x, M - 2);\n}\nint f[N], b[N];\nvoid binomialCoeff() {\n\tf[0] = 1;\n\tfor (int i = 1; i <= N - 1; i++)\n\t\tf[i] = mul(f[i - 1], i);\n\tb[N - 1] = powM(f[N - 1], M - 2);\n\tfor (int i = N - 2; i >= 0; i--)\n\t\tb[i] = mul(b[i + 1], i + 1);\n}\nint C(int n, int r) {\n\treturn (r > n ? 0LL : mul(f[n], mul(b[r], b[n - r])));\n}",
									"format": "snippet",
									"keep_prefix": false,
									"must_insert": false,
									"trigger": "Modulo Combinatorics"
								}
							],
							[
								"insert",
								{
									"characters": "\n"
								}
							]
						]
					},
					"BwAAAC8CAAAAAAAAMAIAAAAAAAAAAAAAMAIAAAAAAAAxAgAAAAAAAAAAAAAxAgAAAAAAADICAAAAAAAAAAAAADICAAAAAAAAMwIAAAAAAAAAAAAALwIAAAAAAAAvAgAAAAAAAAQAAABtb2R1LwIAAAAAAAC0BAAAAAAAAAAAAAC0BAAAAAAAALUEAAAAAAAAAAAAAA",
					"BwAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAALwIAAAAAAAAvAgAAAAAAAAAAAAAAAPC/"
				],
				[
					6,
					1,
					"reindent",
					null,
					"AQAAAAAAAAAAAAAAAQAAAAAAAAAAAAAA",
					"BAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/"
				],
				[
					7,
					2,
					"left_delete",
					null,
					"AQAAAAAAAAAAAAAAAAAAAAAAAAABAAAACQ",
					"BAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAQAAAAAAAAABAAAAAAAAAAAAAAAAAPC/"
				],
				[
					8,
					1,
					"insert",
					{
						"characters": "mul"
					},
					"AwAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAQAAAAAAAAACAAAAAAAAAAAAAAACAAAAAAAAAAMAAAAAAAAAAAAAAA",
					"BAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/"
				],
				[
					10,
					1,
					"delete_word",
					{
						"forward": false
					},
					"AQAAAAAAAAAAAAAAAAAAAAAAAAADAAAAbXVs",
					"BAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAwAAAAAAAAADAAAAAAAAAAAAAAAAAPC/"
				],
				[
					11,
					1,
					"insert",
					{
						"characters": "modu"
					},
					"BAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAQAAAAAAAAACAAAAAAAAAAAAAAACAAAAAAAAAAMAAAAAAAAAAAAAAAMAAAAAAAAABAAAAAAAAAAAAAAA",
					"BAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/"
				],
				[
					12,
					1,
					"insert_completion",
					{
						"completion": "const int M    =     1e9 + 7;\nint add(int a, int b) {\n\ta += b;\n\treturn (a >= M ? a - M : a);\n}\nint mul(int a, int b) {\n\treturn (a * b) % M;\n}\nint sub(int a, int b) {\n\treturn (a - b + M) % M;\n}\nint powM(int b, int p) {\n\tint r = 1;\n\tfor (; p; b = mul(b, b), p >>= 1)\n\t\tif (p & 1)\n\t\t\tr = mul(r, b);\n\treturn r;\n}\nint invM(int x) {\n\treturn powM(x, M - 2);\n}\nint f[N], b[N];\nvoid binomialCoeff() {\n\tf[0] = 1;\n\tfor (int i = 1; i <= N - 1; i++)\n\t\tf[i] = mul(f[i - 1], i);\n\tb[N - 1] = powM(f[N - 1], M - 2);\n\tfor (int i = N - 2; i >= 0; i--)\n\t\tb[i] = mul(b[i + 1], i + 1);\n}\nint C(int n, int r) {\n\treturn (r > n ? 0LL : mul(f[n], mul(b[r], b[n - r])));\n}",
						"format": "snippet",
						"keep_prefix": false,
						"must_insert": false,
						"trigger": "Modulo Combinatorics"
					},
					"AgAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAbW9kdQAAAAAAAAAAhQIAAAAAAAAAAAAA",
					"BAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPC/AAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8L8AAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwvwAAAAABAAAABAAAAAAAAAAEAAAAAAAAAAAAAAAAAPC/"
				]
			]
		},
		{
			"contents": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"name\": \"NLP_CS60075_A21_Assn1_18IE10001.ipynb\",\n      \"provenance\": [],\n      \"collapsed_sections\": [],\n      \"toc_visible\": true,\n      \"include_colab_link\": true\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"language_info\": {\n      \"name\": \"python\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"view-in-github\",\n        \"colab_type\": \"text\"\n      },\n      \"source\": [\n        \"<a href=\\\"https://colab.research.google.com/github/AbhJ/NLPAssignments/blob/main/NLP_CS60075_A21_Assn1_18IE10001.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"4Z_wN2v1RT1F\"\n      },\n      \"source\": [\n        \"# **Assignment-1 for CS60075: Natural Language Processing**\\n\",\n        \"\\n\",\n        \"#### Instructor : Prof. Sudeshna Sarkar\\n\",\n        \"\\n\",\n        \"#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\\n\",\n        \"\\n\",\n        \"#### Date of Announcement: 4th Sept, 2021\\n\",\n        \"#### Deadline for Submission: 11.59pm on Sunday, 12th Sept, 2021 \\n\",\n        \"\\n\",\n        \"#### (**NOTE**: Submit a .zip file, containing this .ipynb file, named as `<Your_Roll_Number>_Assn1_NLP_A21.ipynb` and the raw text corpus named `<Your_Roll_Number>_Assn1_rawCorpus.txt`. For example, if your roll number is 20XX12Y45, name the .ipynb file as `20XX12Y45_Assn1_NLP_A21.ipynb`. Name the .zip as `<Your_Roll_Number>_Assn1_NLP_A21.zip`. Write your code in the respective designated portion of the .ipynb. Also before submitting, make sure that all the outputs of your code are present in the .ipynb file itself.)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"3a35tmEySCx7\"\n      },\n      \"source\": [\n        \"### **Submission Details:**\\n\",\n        \"Name: Abhijay Mitra\\n\",\n        \"\\n\",\n        \"Roll No.: 18IE10001\\n\",\n        \"\\n\",\n        \"Department: Electrical Engineering\\n\",\n        \"\\n\",\n        \"Email-ID: mitraabhijay@gmail.com\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"9weHMmyd8fnq\"\n      },\n      \"source\": [\n        \"## **Reading a Raw Text Corpus**\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"DmSy_LOK2aGQ\"\n      },\n      \"source\": [\n        \"Retrieve & save raw corpus\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"rku6rV2ORpZA\"\n      },\n      \"source\": [\n        \"# To construct your corpus, retrieve (through Python code) Chapter I to Chapter X,\\n\",\n        \"# both inclusive, from the link below:\\n\",\n        \"# \\\"https://www.gutenberg.org/files/730/730-0.txt\\\"\\n\",\n        \"# Save this corpus in a text file, named as 'rawCorpus.txt'\\n\",\n        \"# Print the total number of characters in the text file \\n\",\n        \"\\n\",\n        \"# *** Write code ***\\n\",\n        \"\\n\",\n        \"from urllib.request import urlopen\\n\",\n        \"with urlopen('https://www.gutenberg.org/files/730/730-0.txt') as url:\\n\",\n        \"    textFile = url.read().decode('utf8');\\n\",\n        \"insideFlag = 0\\n\",\n        \"\\n\",\n        \"\\n\",\n        \"startPos = textFile.find('CHAPTER I')\\n\",\n        \"endinPos = textFile.find('CHAPTER XI')\\n\",\n        \"\\n\",\n        \"text = textFile[startPos: endinPos] \\n\",\n        \"\\n\",\n        \"corpusFile = open('rawCorpus.txt', 'w')\\n\",\n        \"corpusFile.write(text)\\n\",\n        \"corpusFile.close()\"\n      ],\n      \"execution_count\": 26,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"6KZIOy0Y2hzQ\"\n      },\n      \"source\": [\n        \"Read the corpus\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"DsdBJa_l2l7g\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"5ccf726f-ff68-4cc6-d97b-ad079b22c955\"\n      },\n      \"source\": [\n        \"# Read the corpus from rawCorpus.txt, in a variable `rawReadCorpus`\\n\",\n        \"# *** Write code ***\\n\",\n        \"\\n\",\n        \"rawReadCorpus = open('rawCorpus.txt', 'r').read()\\n\",\n        \"\\n\",\n        \"print (\\\"Total # of characters in read dataset: {}\\\".format(len(rawReadCorpus)))\"\n      ],\n      \"execution_count\": 27,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Total # of characters in read dataset: 148717\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"xhkmGsSoV0zG\"\n      },\n      \"source\": [\n        \"## **Installing NLTK**\\n\",\n        \"\\n\",\n        \"The Natural Language Toolkit ([NLTK](https://www.nltk.org/)) is a Python module that is intended to support research and teaching in NLP or closely related areas. \\n\",\n        \"\\n\",\n        \"Detailed installation instructions to install NLTK can be found at this [link](https://www.nltk.org/install.html).\\n\",\n        \"\\n\",\n        \"To ensure uniformity, we suggest to use **python3**. You can download Anaconda3 and create a separate environment to do this assignment, eg.\\n\",\n        \"```bash\\n\",\n        \"conda create -n myenv python=3.6\\n\",\n        \"conda activate myenv\\n\",\n        \"```\\n\",\n        \"\\n\",\n        \"The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. Subsequently, you can install NLTK through the following commands:\\n\",\n        \"```bash\\n\",\n        \"sudo pip3 install nltk \\n\",\n        \"python3 \\n\",\n        \"nltk.download()\\n\",\n        \"```\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"utKtZeHq4N98\"\n      },\n      \"source\": [\n        \"## **Preprocessing the corpus**\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"_gP9ihZnn_9W\"\n      },\n      \"source\": [\n        \"\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"5-LSUX__82Ff\"\n      },\n      \"source\": [\n        \"**Tokenize into words and sentences, using NLTK library:** Using the NLTK modules imported above, retrieve a case-insensitive preprocessed model. Make sure to take care of words like \\\"\\\\_will\\\\_\\\" (that should ideally appear as \\\"will\\\"), \\\"wouldn't\\\" (that should ideally appear as a single word, and not multiple tokens) and other occurences of special cases that you find in the raw corpus. \"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"2g7eO4Dm4jIn\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"458a3404-aec1-4ae5-8b5d-e350161521ca\"\n      },\n      \"source\": [\n        \"# Importing modules\\n\",\n        \"import nltk\\n\",\n        \"nltk.download('punkt') # For tokenizers\\n\",\n        \"from nltk.tokenize import word_tokenize,sent_tokenize\"\n      ],\n      \"execution_count\": 28,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"[nltk_data] Downloading package punkt to /root/nltk_data...\\n\",\n            \"[nltk_data]   Package punkt is already up-to-date!\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"cWIzYXyz9Zt_\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"9916bd2f-3460-4530-bcc1-3db9f3bbf9fc\"\n      },\n      \"source\": [\n        \"# *** Write code for preprocessing the corpus ***\\n\",\n        \"\\n\",\n        \"text = rawReadCorpus.replace('\\\\n', ' ')\\n\",\n        \"\\n\",\n        \"def tokenisationMethodForSent(text):\\n\",\n        \"\\n\",\n        \"    unwantedPunctuations = '\\\\\\\"’—”“\\\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}'\\n\",\n        \"\\n\",\n        \"    text = \\\"\\\".join([char for char in text if char not in unwantedPunctuations])\\n\",\n        \"\\n\",\n        \"    text = sent_tokenize(text)\\n\",\n        \"\\n\",\n        \"    return [item.lower() for item in text]\\n\",\n        \"\\n\",\n        \"def tokenisationMethodForWord(text):\\n\",\n        \"\\n\",\n        \"    unwantedPunctuations = '\\\\\\\"’—”“\\\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}'\\n\",\n        \"\\n\",\n        \"    text = \\\"\\\".join([char for char in text if char not in unwantedPunctuations])\\n\",\n        \"\\n\",\n        \"    text = text.replace('.', ' ')\\n\",\n        \"    text = text.replace('?', ' ')\\n\",\n        \"    text = text.replace('!', ' ')\\n\",\n        \"    \\n\",\n        \"    text = word_tokenize(text)\\n\",\n        \"\\n\",\n        \"    return [item.lower() for item in text]\\n\",\n        \"\\n\",\n        \"textTokenisedToSent = tokenisationMethodForSent(text)\\n\",\n        \"textTokenisedToWord = tokenisationMethodForWord(text)\\n\",\n        \"\\n\",\n        \"# Print first 5 sentences of your preprocessed corpus *** Write code ***\\n\",\n        \"# Print first 5 words/tokens of your preprocessed corpus *** Write code ***\\n\",\n        \"\\n\",\n        \"print('First 5 sentences are: ', textTokenisedToSent[:5])\\n\",\n        \"print('First 5 words are:     ', textTokenisedToWord[:5])\"\n      ],\n      \"execution_count\": 29,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"First 5 sentences are:  ['chapter i.', 'treats of the place where oliver twist was born and of the circumstances attending his birth   among other public buildings in a certain town which for many reasons it will be prudent to refrain from mentioning and to which i will assign no fictitious name there is one anciently common to most towns great or small to wit a workhouse and in this workhouse was born on a day and date which i need not trouble myself to repeat inasmuch as it can be of no possible consequence to the reader in this stage of the business at all events the item of mortality whose name is prefixed to the head of this chapter.', 'for a long time after it was ushered into this world of sorrow and trouble by the parish surgeon it remained a matter of considerable doubt whether the child would survive to bear any name at all in which case it is somewhat more than probable that these memoirs would never have appeared or if they had that being comprised within a couple of pages they would have possessed the inestimable merit of being the most concise and faithful specimen of biography extant in the literature of any age or country.', 'although i am not disposed to maintain that the being born in a workhouse is in itself the most fortunate and enviable circumstance that can possibly befall a human being i do mean to say that in this particular instance it was the best thing for oliver twist that could by possibility have occurred.', 'the fact is that there was considerable difficulty in inducing oliver to take upon himself the office of respirationa troublesome practice but one which custom has rendered necessary to our easy existence and for some time he lay gasping on a little flock mattress rather unequally poised between this world and the next the balance being decidedly in favour of the latter.']\\n\",\n            \"First 5 words are:      ['chapter', 'i', 'treats', 'of', 'the']\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"eZ75_a1QL70J\"\n      },\n      \"source\": [\n        \"**Perform the following tasks for the given corpus:**\\n\",\n        \"1. Print the average number of tokens per sentence.\\n\",\n        \"2. Print the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\\n\",\n        \"3. Print the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive).\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"pyG0g3oSADmV\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"f7db4603-c30e-46f2-9ee0-f1352c1618f8\"\n      },\n      \"source\": [\n        \"# Importing modules\\n\",\n        \"nltk.download('stopwords')\\n\",\n        \"from nltk.corpus import stopwords\"\n      ],\n      \"execution_count\": 30,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"[nltk_data] Downloading package stopwords to /root/nltk_data...\\n\",\n            \"[nltk_data]   Package stopwords is already up-to-date!\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"ydHIxC7lG7Py\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"11bd372c-00d5-49b9-a71b-20784ff11330\"\n      },\n      \"source\": [\n        \"# *** Write code for the 2 tasks above ***\\n\",\n        \"\\n\",\n        \"print('1. The average number of tokens per sentence is: ' + str(len(textTokenisedToWord) / len(textTokenisedToSent)))\\n\",\n        \"\\n\",\n        \"maxLengthWithOliver = float('-inf')\\n\",\n        \"minLengthWithOliver = float('inf')\\n\",\n        \"\\n\",\n        \"for sent in textTokenisedToSent:\\n\",\n        \"    if 'Oliver'.lower() in sent:\\n\",\n        \"        maxLengthWithOliver = max(maxLengthWithOliver, len(sent))\\n\",\n        \"        minLengthWithOliver = min(minLengthWithOliver, len(sent))\\n\",\n        \"\\n\",\n        \"print('2.a. Length of the longest sentence with word \\\\'Oliver\\\\' is: ' + str(maxLengthWithOliver) + ' characters')\\n\",\n        \"print('2.b. Length of the shortest sentence with word \\\\'Oliver\\\\' is: ' + str(minLengthWithOliver) + ' characters')\\n\",\n        \"\\n\",\n        \"# removing stopwords\\n\",\n        \"tokensWithoutSw = [word for word in textTokenisedToWord if word not in stopwords.words('english')]\\n\",\n        \"\\n\",\n        \"# only storing unique words\\n\",\n        \"tokensUnique    = set(tokensWithoutSw)\\n\",\n        \"\\n\",\n        \"print('3. The number of unique tokens in the corpus, after stopword removal is: ' + str(len(tokensUnique)))\"\n      ],\n      \"execution_count\": 31,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"1. The average number of tokens per sentence is: 19.928134556574925\\n\",\n            \"2.a. Length of the longest sentence with word 'Oliver' is: 631 characters\\n\",\n            \"2.b. Length of the shortest sentence with word 'Oliver' is: 7 characters\\n\",\n            \"3. The number of unique tokens in the corpus, after stopword removal is: 4206\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"X5RiDR7TJjKX\"\n      },\n      \"source\": [\n        \"## **Language Modeling**\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"UJeTSt8HM95L\"\n      },\n      \"source\": [\n        \"### Task: In this sub-task, you are expected to carry out the following tasks:\\n\",\n        \"\\n\",\n        \"1. **Create the following language models** on the given corpus: <br>\\n\",\n        \"    i.   Unigram <br>\\n\",\n        \"    ii.  Bigram <br>\\n\",\n        \"    iii. Trigram <br>\\n\",\n        \"\\n\",\n        \"2. **List the top 10 bigrams, trigrams**\\n\",\n        \"(Additionally remove those items which contain only articles, prepositions, determiners eg. \\\"of the\\\", \\\"in a\\\", etc. List top-10 bigrams/trigrams in both the original and processed models).\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"DlPXGvVaR-ka\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"be2092aa-cac3-4219-ad23-3ec97c183f2b\"\n      },\n      \"source\": [\n        \"from nltk.util import ngrams\\n\",\n        \"\\n\",\n        \"unigrams=[]\\n\",\n        \"unigrams_Processed=[]\\n\",\n        \"bigrams=[]\\n\",\n        \"trigrams=[]\\n\",\n        \"tokenized_text = []\\n\",\n        \"\\n\",\n        \"stop_words = set(stopwords.words('english'))\\n\",\n        \"\\n\",\n        \"for content in textTokenisedToSent:\\n\",\n        \"    # *** Write code ***\\n\",\n        \"\\n\",\n        \"    sequence = tokenisationMethodForWord(content)\\n\",\n        \"\\n\",\n        \"    for word in sequence:\\n\",\n        \"        if (word == '.'):\\n\",\n        \"            sequence.remove(word) \\n\",\n        \"        else:\\n\",\n        \"            unigrams.append(word)\\n\",\n        \"            if word not in stop_words:\\n\",\n        \"                unigrams_Processed.append(word)\\n\",\n        \"\\n\",\n        \"    tokenized_text.append(sequence) \\n\",\n        \"\\n\",\n        \"    if len(content.split()) > 1:\\n\",\n        \"        bigrams.extend(list(ngrams(sequence, 2)))\\n\",\n        \"    if len(content.split()) > 2:\\n\",\n        \"        trigrams.extend(list(ngrams(sequence, 3)))\\n\",\n        \"\\n\",\n        \"print (\\\"Sample of n-grams:\\\\n\\\" + \\\"-------------------------\\\")\\n\",\n        \"print (\\\"--> UNIGRAMS: \\\\n\\\" + str(unigrams[:5]) + \\\" ...\\\\n\\\")\\n\",\n        \"print (\\\"--> BIGRAMS: \\\\n\\\" + str(bigrams[:5]) + \\\" ...\\\\n\\\")\\n\",\n        \"print (\\\"--> TRIGRAMS: \\\\n\\\" + str(trigrams[:5]) + \\\" ...\\\\n\\\")\\n\",\n        \"\\n\",\n        \"def removal(wordList):\\n\",\n        \"#removes ngrams containing only stopwords\\n\",\n        \"    wordListWithoutSw = []\\n\",\n        \"    for words in wordList:\\n\",\n        \"        flag = 0\\n\",\n        \"        for word in words:\\n\",\n        \"            if word in stop_words:\\n\",\n        \"                flag = flag or 0\\n\",\n        \"            else:\\n\",\n        \"                flag = flag or 1\\n\",\n        \"        if flag == 1:\\n\",\n        \"            wordListWithoutSw.append(words)\\n\",\n        \"    return wordListWithoutSw\\n\",\n        \"\\n\",\n        \"bigrams_Processed = removal(bigrams)\\n\",\n        \"trigrams_Processed = removal(trigrams) \\n\",\n        \"\\n\",\n        \"print (\\\"Sample of n-grams after processing:\\\\n\\\" + \\\"-------------------------\\\")\\n\",\n        \"print (\\\"--> UNIGRAMS: \\\\n\\\" + str(unigrams_Processed[:5]) + \\\" ...\\\\n\\\")\\n\",\n        \"print (\\\"--> BIGRAMS: \\\\n\\\" + str(bigrams_Processed[:5]) + \\\" ...\\\\n\\\")\\n\",\n        \"print (\\\"--> TRIGRAMS: \\\\n\\\" + str(trigrams_Processed[:5]) + \\\" ...\\\\n\\\")\\n\",\n        \"\\n\",\n        \"def get_ngrams_freqDist(n, ngramList):\\n\",\n        \"    #This function computes the frequency corresponding to each ngram in ngramList \\n\",\n        \"    #Here, n=1 for unigram, n=2 for bigram, etc.\\n\",\n        \"    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\\n\",\n        \"    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\\n\",\n        \"    \\n\",\n        \"    # *** Write code ***\\n\",\n        \"    \\n\",\n        \"    return nltk.FreqDist(ngramList)\\n\",\n        \"\\n\",\n        \"unigrams_freqDist = get_ngrams_freqDist(1, unigrams)\\n\",\n        \"unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\\n\",\n        \"bigrams_freqDist = get_ngrams_freqDist(2, bigrams)\\n\",\n        \"bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\\n\",\n        \"trigrams_freqDist = get_ngrams_freqDist(3, trigrams)\\n\",\n        \"trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed)\\n\",\n        \"\\n\",\n        \"print('top 10 unigrams, having highest frequency as in unigrams_freqDist are: ', '')\\n\",\n        \"print(unigrams_freqDist.most_common(10))\\n\",\n        \"\\n\",\n        \"# Print top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\\n\",\n        \"\\n\",\n        \"print('top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist are: ', '')\\n\",\n        \"print(unigrams_Processed_freqDist.most_common(10))\\n\",\n        \"\\n\",\n        \"# Print top 10 bigrams, having highest frequency as in bigrams_freqDist\\n\",\n        \"\\n\",\n        \"print('top 10 bigrams, having highest frequency as in bigrams_freqDist are: ', '')\\n\",\n        \"print(bigrams_freqDist.most_common(10))\\n\",\n        \"\\n\",\n        \"# Print top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\\n\",\n        \"\\n\",\n        \"print('top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist are: ', '')\\n\",\n        \"print(bigrams_Processed_freqDist.most_common(10))\\n\",\n        \"\\n\",\n        \"# Print top 10 trigrams, having highest frequency as in trigrams_freqDist\\n\",\n        \"\\n\",\n        \"print('top 10 trigrams, having highest frequency as in trigrams_freqDist are: ', '')\\n\",\n        \"print(trigrams_freqDist.most_common(10))\\n\",\n        \"\\n\",\n        \"# Print top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\\n\",\n        \"\\n\",\n        \"print('top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist are: ', '')\\n\",\n        \"print(trigrams_Processed_freqDist.most_common(10))\"\n      ],\n      \"execution_count\": 32,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Sample of n-grams:\\n\",\n            \"-------------------------\\n\",\n            \"--> UNIGRAMS: \\n\",\n            \"['chapter', 'i', 'treats', 'of', 'the'] ...\\n\",\n            \"\\n\",\n            \"--> BIGRAMS: \\n\",\n            \"[('chapter', 'i'), ('treats', 'of'), ('of', 'the'), ('the', 'place'), ('place', 'where')] ...\\n\",\n            \"\\n\",\n            \"--> TRIGRAMS: \\n\",\n            \"[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\\n\",\n            \"\\n\",\n            \"Sample of n-grams after processing:\\n\",\n            \"-------------------------\\n\",\n            \"--> UNIGRAMS: \\n\",\n            \"['chapter', 'treats', 'place', 'oliver', 'twist'] ...\\n\",\n            \"\\n\",\n            \"--> BIGRAMS: \\n\",\n            \"[('chapter', 'i'), ('treats', 'of'), ('the', 'place'), ('place', 'where'), ('where', 'oliver')] ...\\n\",\n            \"\\n\",\n            \"--> TRIGRAMS: \\n\",\n            \"[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\\n\",\n            \"\\n\",\n            \"top 10 unigrams, having highest frequency as in unigrams_freqDist are:  \\n\",\n            \"[('the', 1701), ('and', 857), ('a', 713), ('of', 673), ('to', 618), ('his', 455), ('he', 449), ('in', 441), ('was', 368), ('oliver', 278)]\\n\",\n            \"top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist are:  \\n\",\n            \"[('oliver', 278), ('said', 212), ('mr', 191), ('bumble', 123), ('gentleman', 102), ('old', 89), ('would', 77), ('sowerberry', 77), ('replied', 74), ('boy', 74)]\\n\",\n            \"top 10 bigrams, having highest frequency as in bigrams_freqDist are:  \\n\",\n            \"[(('of', 'the'), 162), (('in', 'the'), 127), (('mr', 'bumble'), 108), (('to', 'the'), 91), (('said', 'the'), 90), (('he', 'had'), 67), (('he', 'was'), 62), (('on', 'the'), 60), (('in', 'a'), 55), (('with', 'a'), 54)]\\n\",\n            \"top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist are:  \\n\",\n            \"[(('mr', 'bumble'), 108), (('said', 'the'), 90), (('the', 'old'), 53), (('old', 'gentleman'), 39), (('the', 'undertaker'), 36), (('said', 'mr'), 35), (('the', 'boy'), 35), (('the', 'gentleman'), 33), (('mrs', 'sowerberry'), 33), (('the', 'jew'), 33)]\\n\",\n            \"top 10 trigrams, having highest frequency as in trigrams_freqDist are:  \\n\",\n            \"[(('the', 'old', 'gentleman'), 29), (('gentleman', 'in', 'the'), 22), (('the', 'gentleman', 'in'), 20), (('the', 'white', 'waistcoat'), 20), (('said', 'mr', 'bumble'), 19), (('in', 'the', 'white'), 18), (('said', 'the', 'gentleman'), 14), (('said', 'the', 'undertaker'), 14), (('said', 'the', 'jew'), 14), (('sir', 'replied', 'oliver'), 12)]\\n\",\n            \"top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist are:  \\n\",\n            \"[(('the', 'old', 'gentleman'), 29), (('gentleman', 'in', 'the'), 22), (('the', 'gentleman', 'in'), 20), (('the', 'white', 'waistcoat'), 20), (('said', 'mr', 'bumble'), 19), (('in', 'the', 'white'), 18), (('said', 'the', 'gentleman'), 14), (('said', 'the', 'undertaker'), 14), (('said', 'the', 'jew'), 14), (('sir', 'replied', 'oliver'), 12)]\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"Lqu8nVV7NREo\"\n      },\n      \"source\": [\n        \"## **Next three words' Prediction using Smoothed Models**\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"a2vnIM26b2WA\"\n      },\n      \"source\": [\n        \"For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\\n\",\n        \"That is, pretend we saw each word one more time than we did.\\n\",\n        \"\\n\",\n        \"You have two tasks here.\\n\",\n        \"\\n\",\n        \"First, compute the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist you calculated above (use the unprocessed models). Second, using these smoothed models, predict the next 3 possible word sequences for testSent1, testSent2 and testSent3, using your smoothed models.\\n\",\n        \"\\n\",\n        \"As for example, for the string 'Raj has a' the answers can be as below: \\n\",\n        \"\\n\",\n        \"(1) Raj has a **beautiful red car**\\n\",\n        \"\\n\",\n        \"(2) Raj has a **charismatic magnetic personality**\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"qAGB1_S8NThy\"\n      },\n      \"source\": [\n        \"testSent1 = \\\"There was a sudden jerk, a terrific convulsion of the limbs; and there he\\\"\\n\",\n        \"testSent2 = \\\"They made room for the stranger, but he sat down\\\"\\n\",\n        \"testSent3 = \\\"The hungry and destitute situation of the infant orphan was duly reported by\\\"\"\n      ],\n      \"execution_count\": 33,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"tWd8qiSMgrtT\",\n        \"colab\": {\n          \"base_uri\": \"https://localhost:8080/\"\n        },\n        \"outputId\": \"22294a8e-4ac8-499e-85cb-c6fa834973e6\"\n      },\n      \"source\": [\n        \"from nltk.probability import LaplaceProbDist\\n\",\n        \"from nltk.probability import FreqDist\\n\",\n        \"\\n\",\n        \"nGrams = {1:[], 2:[], 3:[]}\\n\",\n        \"for i in range(3):\\n\",\n        \"    for each in tokenized_text:\\n\",\n        \"        if len(each) > i:\\n\",\n        \"            for j in ngrams(each, i+1):\\n\",\n        \"                nGrams[i+1].append(j);\\n\",\n        \"nGramsVoc = {1:set([]), 2:set([]), 3:set([])}\\n\",\n        \"for i in range(3):\\n\",\n        \"    for gram in nGrams[i+1]:\\n\",\n        \"        if gram not in nGramsVoc[i+1]:\\n\",\n        \"            nGramsVoc[i+1].add(gram)\\n\",\n        \"total_ngrams = {1:-1, 2:-1, 3:-1}\\n\",\n        \"total_voc = {1:-1, 2:-1, 3:-1}\\n\",\n        \"for i in range(3):\\n\",\n        \"    total_ngrams[i+1] = len(nGrams[i+1])\\n\",\n        \"    total_voc[i+1] = len(nGramsVoc[i+1])                       \\n\",\n        \"ngrams_prob = {1:[], 2:[], 3:[]}\\n\",\n        \"for i in range(3):\\n\",\n        \"    for ngram in nGramsVoc[i+1]:\\n\",\n        \"        tlist = [ngram]\\n\",\n        \"        tlist.append(nGrams[i+1].count(ngram))\\n\",\n        \"        ngrams_prob[i+1].append(tlist)\\n\",\n        \"for i in range(3):\\n\",\n        \"    for ngram in ngrams_prob[i+1]:\\n\",\n        \"        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1]) \\n\",\n        \"\\n\",\n        \"#Prints top 10 unigram, bigram, trigram after smoothing\\n\",\n        \"print(\\\"Most common n-grams without stopword removal and with add-1 smoothing: \\\\n\\\")\\n\",\n        \"for i in range(3):\\n\",\n        \"    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\\n\",\n        \"    \\n\",\n        \"print (\\\"Most common unigrams: \\\", str(ngrams_prob[1][:10]))\\n\",\n        \"print (\\\"\\\\nMost common bigrams: \\\", str(ngrams_prob[2][:10]))\\n\",\n        \"print (\\\"\\\\nMost common trigrams: \\\", str(ngrams_prob[3][:10]))\\n\",\n        \"\\n\",\n        \"#smoothed models without stopwords removed are used\\n\",\n        \"token_1 = word_tokenize(testSent1)\\n\",\n        \"token_2 = word_tokenize(testSent2)\\n\",\n        \"token_3 = word_tokenize(testSent3)\\n\",\n        \"ngram_1 = {1:[], 2:[], 3:[]}   #to store the n-grams formed  \\n\",\n        \"ngram_2 = {1:[], 2:[], 3:[]}\\n\",\n        \"ngram_3 = {1:[], 2:[], 3:[]}\\n\",\n        \"for i in range(3):\\n\",\n        \"    ngram_1[i+1] = list(ngrams(token_1, i+1))[-1]\\n\",\n        \"    ngram_2[i+1] = list(ngrams(token_2, i+1))[-1]\\n\",\n        \"    ngram_3[i+1] = list(ngrams(token_3, i+1))[-1]\\n\",\n        \"\\n\",\n        \"for i in range(3):\\n\",\n        \"    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\\n\",\n        \"    \\n\",\n        \"pred_1 = {1:[], 2:[], 3:[]}\\n\",\n        \"for i in range(3):\\n\",\n        \"    count = 0\\n\",\n        \"    if len(ngrams_prob) > i + 2:\\n\",\n        \"        for each in ngrams_prob[i+2]:\\n\",\n        \"            if each[0][:-1] == ngram_1[i+1]:      \\n\",\n        \"                count +=1\\n\",\n        \"                pred_1[i+1].append(each[0][-1])\\n\",\n        \"                if count == 3:\\n\",\n        \"                    break\\n\",\n        \"for i in range(3):\\n\",\n        \"    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\\n\",\n        \"    \\n\",\n        \"pred_2 = {1:[], 2:[], 3:[]}\\n\",\n        \"for i in range(3):\\n\",\n        \"    count = 0\\n\",\n        \"    if len(ngrams_prob) > i + 2:\\n\",\n        \"        for each in ngrams_prob[i+2]:\\n\",\n        \"            if each[0][:-1] == ngram_2[i+1]:\\n\",\n        \"                count +=1\\n\",\n        \"                pred_2[i+1].append(each[0][-1])\\n\",\n        \"                if count == 3:\\n\",\n        \"                    break\\n\",\n        \"\\n\",\n        \"pred_3 = {1:[], 2:[], 3:[]}\\n\",\n        \"for i in range(3):\\n\",\n        \"    count = 0\\n\",\n        \"    if len(ngrams_prob) > i + 2:\\n\",\n        \"        for each in ngrams_prob[i+2]:\\n\",\n        \"            if each[0][:-1] == ngram_3[i+1]:\\n\",\n        \"                count +=1\\n\",\n        \"                pred_3[i+1].append(each[0][-1])\\n\",\n        \"                if count == 3:\\n\",\n        \"                    break\\n\",\n        \"\\n\",\n        \"print(\\\"Next word predictions for the strings using the probability models of bigrams, trigrams\\\\n\\\")\\n\",\n        \"print(\\\"String 1: \\\" + testSent1)\\n\",\n        \"print(\\\"Bigram model predictions: {}\\\" .format(pred_1[1]))\\n\",\n        \"print(\\\"String 2: \\\" + testSent2)\\n\",\n        \"print(\\\"Bigram model predictions: {}\\\" .format(pred_2[1]))\\n\",\n        \"print(\\\"String 3: \\\" + testSent3)\\n\",\n        \"print(\\\"Bigram model predictions: {}\\\" .format(pred_3[1]))\\n\"\n      ],\n      \"execution_count\": 34,\n      \"outputs\": [\n        {\n          \"output_type\": \"stream\",\n          \"name\": \"stdout\",\n          \"text\": [\n            \"Most common n-grams without stopword removal and with add-1 smoothing: \\n\",\n            \"\\n\",\n            \"Most common unigrams:  [[('the',), 0.056003422065743144], [('and',), 0.02823204238096805], [('a',), 0.023493797505840543], [('of',), 0.02217761837386068], [('to',), 0.02036787206738837], [('his',), 0.015004442104570432], [('he',), 0.014807015234773452], [('in',), 0.014543779408377481], [('was',), 0.012141752492514231], [('oliver',), 0.009180349445559541]]\\n\",\n            \"\\n\",\n            \"Most common bigrams:  [[('of', 'the'), 0.003980755610911667], [('in', 'the'), 0.0031259921361760324], [('mr', 'bumble'), 0.0026619776784624024], [('to', 'the'), 0.0022468068478765234], [('said', 'the'), 0.002222385034312648], [('he', 'had'), 0.0016606833223435172], [('he', 'was'), 0.001538574254524141], [('on', 'the'), 0.0014897306273963904], [('in', 'a'), 0.0013676215595770141], [('with', 'a'), 0.0013431997460131388]]\\n\",\n            \"\\n\",\n            \"Most common trigrams:  [[('the', 'old', 'gentleman'), 0.0006625880689975043], [('gentleman', 'in', 'the'), 0.0005079841862314199], [('the', 'gentleman', 'in'), 0.000463811648298253], [('the', 'white', 'waistcoat'), 0.000463811648298253], [('said', 'mr', 'bumble'), 0.0004417253793316695], [('in', 'the', 'white'), 0.000419639110365086], [('said', 'the', 'jew'), 0.00033129403449875215], [('said', 'the', 'gentleman'), 0.00033129403449875215], [('said', 'the', 'undertaker'), 0.00033129403449875215], [('sir', 'replied', 'oliver'), 0.0002871214965655852]]\\n\",\n            \"Next word predictions for the strings using the probability models of bigrams, trigrams\\n\",\n            \"\\n\",\n            \"String 1: There was a sudden jerk, a terrific convulsion of the limbs; and there he\\n\",\n            \"Bigram model predictions: ['had', 'was', 'would']\\n\",\n            \"String 2: They made room for the stranger, but he sat down\\n\",\n            \"Bigram model predictions: ['the', 'a', 'to']\\n\",\n            \"String 3: The hungry and destitute situation of the infant orphan was duly reported by\\n\",\n            \"Bigram model predictions: ['the', 'a', 'this']\\n\"\n          ]\n        }\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"rxfeaacTdO6h\"\n      },\n      \"source\": [\n        \"Check the presence of these sentences in the original corpus at https://www.gutenberg.org/files/730/730-0.txt . How did your smoothed models perform in comparison to the original sentences? Compare them below.\\n\",\n        \"\\n\",\n        \"Did you notice something special about testSent3, in comparison to testSent1 and testSent2? If yes, what is it? Can you explain it?\\n\",\n        \"\\n\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"LQ-AcC9liDAI\"\n      },\n      \"source\": [\n        \"\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"nFMkW9hKecxK\"\n      },\n      \"source\": [\n        \"  - - - - - - - - - -\\n\",\n        \"  ** Answer here ** \\n\",\n        \"   - - - - - - - - - -\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"vVBMcaAJXR9S\"\n      },\n      \"source\": [\n        \"Which of the three models you generated above (unigram, bigram, trigram) is better in terms of **perplexity**, for the three test sentences (unseen data)? Write a piece of code to justify your answer. \"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"AAPa1OVZX8uN\"\n      },\n      \"source\": [\n        \"  - - - - - - - - - -\\n\",\n        \"  ** Answer here ** \\n\",\n        \"   - - - - - - - - - -\"\n      ]\n    }\n  ]\n}",
			"file": "/home/abj/Documents/7th sem notes/nlp/NLPAssignments/NLP_CS60075_A21_Assn1_18IE10001.ipynb",
			"file_size": 34473,
			"file_write_time": 132758477524278395,
			"settings":
			{
				"buffer_size": 34457,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/abj/Documents/text_docs/time.txt",
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "Packages/C++/C++ Single File.sublime-build",
	"build_system_choices":
	[
		[
			[
				[
					"Packages/C++/C++ Single File.sublime-build",
					""
				],
				[
					"Packages/C++/C++ Single File.sublime-build",
					"Run"
				]
			],
			[
				"Packages/C++/C++ Single File.sublime-build",
				"Run"
			]
		],
		[
			[
				[
					"Packages/User/cp.sublime-build",
					""
				],
				[
					"Packages/User/cp.sublime-build",
					"Variant Run"
				]
			],
			[
				"Packages/User/cp.sublime-build",
				""
			]
		]
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				":w",
				":w - Save"
			],
			[
				"git a",
				"Git: Add All"
			],
			[
				"git add ",
				"Git: Add All"
			],
			[
				"git pus",
				"Git: Push Current Branch"
			],
			[
				"git co",
				"Git: Quick Commit (repo)"
			],
			[
				"git add",
				"Git: Add All"
			],
			[
				"git ad",
				"Git: Add All"
			],
			[
				":q",
				"Git: Quick Commit (repo)"
			],
			[
				"ope",
				"PackageResourceViewer: Open Resource"
			],
			[
				"wakat",
				"WakaTime: Open Dashboard"
			],
			[
				"upp",
				"Convert Case: Upper Case"
			],
			[
				":W",
				":w - Save"
			],
			[
				":push",
				"Git: Push"
			],
			[
				"git q",
				"Git: Quick Commit (repo)"
			],
			[
				"git ",
				"Git: Push"
			],
			[
				":open res",
				"PackageResourceViewer: Open Resource"
			],
			[
				"waka",
				"WakaTime: Open Dashboard"
			],
			[
				"inst",
				"Package Control: Install Package"
			],
			[
				"track",
				"CodeTimeTracker: Open Dashboard"
			],
			[
				"colo",
				"UI: Select Color Scheme"
			],
			[
				":the",
				"UI: Select Theme"
			],
			[
				":th",
				"UI: Select Theme"
			],
			[
				"",
				"Code Folding: Fold"
			],
			[
				"theme",
				"UI: Select Theme"
			],
			[
				"you",
				"YouCompleteMe: Show file information"
			],
			[
				":",
				":w - Save"
			],
			[
				"remo",
				"Package Control: Remove Package"
			],
			[
				"ins",
				"Package Control: Install Package"
			],
			[
				"insa",
				"Package Control: Install Package"
			],
			[
				"col",
				"UI: Select Color Scheme"
			],
			[
				"git p",
				"Git: Push"
			],
			[
				"git com",
				"Git: Quick Commit (repo)"
			],
			[
				"ui",
				"UI: Select Theme"
			],
			[
				"insta",
				"Package Control: Install Package"
			],
			[
				"color",
				"UI: Select Color Scheme"
			],
			[
				"THEME",
				"UI: Select Theme"
			],
			[
				"rme",
				"Package Control: Remove Package"
			],
			[
				"pdf",
				"LaTeXing: Jump To PDF"
			],
			[
				"late",
				"LaTeXing: Open PDF"
			],
			[
				"gi",
				"Git: Push"
			],
			[
				"git PUS",
				"Git: Push"
			],
			[
				"git qui",
				"Git: Quick Commit (repo)"
			],
			[
				"op",
				"PackageResourceViewer: Open Resource"
			],
			[
				"instal",
				"Package Control: Install Package"
			],
			[
				"colo\\",
				"UI: Select Color Scheme"
			],
			[
				"them",
				"UI: Select Theme"
			],
			[
				"the",
				"UI: Select Theme"
			],
			[
				"mat",
				"Material Theme: Activate theme"
			],
			[
				"GIT pus",
				"Git: Push"
			],
			[
				"GIT co",
				"Git: Commit"
			],
			[
				"GIT add",
				"Git: Add All"
			],
			[
				"git comm",
				"Git: Quick Commit (repo)"
			],
			[
				"tra",
				"Trim Trailing White Space"
			],
			[
				"INSTA",
				"Package Control: Install Package"
			],
			[
				"upper",
				"Convert Case: Upper Case"
			],
			[
				"us",
				"UI: Select Theme"
			],
			[
				"LOW",
				"Convert Case: Lower Case"
			],
			[
				"uppe",
				"Convert Case: Upper Case"
			],
			[
				"ww",
				"Word Wrap: Toggle"
			],
			[
				"isnta",
				"Package Control: Install Package"
			],
			[
				"rem",
				"Package Control: Remove Package"
			],
			[
				"re",
				"Package Control: Remove Package"
			],
			[
				"uope",
				"PackageResourceViewer: Open Resource"
			],
			[
				"remov",
				"Package Control: Remove Package"
			],
			[
				"reso",
				"PackageResourceViewer: Open Resource"
			],
			[
				"highli",
				"HighlightWords: Toggle Settings"
			],
			[
				"docu",
				"Tasks: New document"
			],
			[
				"docume",
				"Tasks: New document"
			],
			[
				"new ",
				"Tasks: New"
			],
			[
				"tesk",
				"Tasks: Open Link"
			],
			[
				"linter",
				"Preferences: SublimeLinter Settings"
			],
			[
				"open",
				"PackageResourceViewer: Open Resource"
			],
			[
				"trac",
				"CodeTimeTracker: Open Dashboard"
			],
			[
				"color s ",
				"UI: Select Color Scheme"
			],
			[
				"open re",
				"PackageResourceViewer: Open Resource"
			],
			[
				"open r",
				"PackageResourceViewer: Open Resource"
			],
			[
				"open reso",
				"PackageResourceViewer: Open Resource"
			],
			[
				"snippe",
				"Snippet: For Loop"
			],
			[
				"revers",
				"Permute Lines: Reverse"
			],
			[
				"sort",
				"Sort Lines"
			],
			[
				"color p",
				"Color Picker"
			],
			[
				"code time",
				"CodeTimeTracker: Open Dashboard"
			],
			[
				"tracke",
				"CodeTimeTracker: Open Dashboard"
			],
			[
				"teacke",
				"CodeTimeTracker: Open Dashboard"
			],
			[
				"codeti",
				"Code Time: Show Tree View"
			],
			[
				"openre",
				"PackageResourceViewer: Open Resource"
			],
			[
				"opw",
				"Package Control: Upgrade/Overwrite All Packages"
			],
			[
				"time",
				"Code Time: Show Tree View"
			],
			[
				"packa",
				"Package Control: Install Package"
			],
			[
				"indent",
				"Indentation: Reindent Lines"
			],
			[
				"inde",
				"Indentation: Reindent Lines"
			],
			[
				"idne",
				"Indentation: Reindent Lines"
			],
			[
				"inden",
				"Indentation: Convert to Tabs"
			],
			[
				"idne'",
				"Indentation: Reindent Lines"
			],
			[
				"indr",
				"Indentation: Reindent Lines"
			],
			[
				"trail",
				"Trim Trailing White Space"
			],
			[
				"package re",
				"PackageResourceViewer: Open Resource"
			],
			[
				"packageres",
				"PackageResourceViewer: Open Resource"
			],
			[
				"packagere",
				"PackageResourceViewer: Open Resource"
			],
			[
				"close ",
				"File: Close All"
			],
			[
				"files",
				"File: Save All"
			],
			[
				"fodl m",
				"Fold Comments"
			],
			[
				"comment",
				"Toggle Comment"
			],
			[
				"fold ",
				"Code Folding: Fold"
			],
			[
				"color highli",
				"Preferences: Color Highlight Settings"
			],
			[
				"sublime",
				"SublimeLinter: Goto Error"
			],
			[
				"settings",
				"Preferences: Settings"
			],
			[
				"wor",
				"Word Wrap: Toggle"
			],
			[
				"alignme",
				"Preferences: Alignment File Settings – User"
			],
			[
				"align",
				"Preferences: Alignment File Settings – User"
			],
			[
				"pa",
				"Package Control: Install Package"
			],
			[
				"permu",
				"Permute Lines: Shuffle"
			],
			[
				"lowee",
				"Convert Case: Lower Case"
			],
			[
				"ui ",
				"UI: Select Color Scheme"
			],
			[
				"ui co",
				"UI: Select Color Scheme"
			],
			[
				"remove",
				"Package Control: Remove Package"
			],
			[
				"ui colo",
				"UI: Select Color Scheme"
			],
			[
				"pac",
				"Package Control: Install Package"
			],
			[
				"select ui",
				"UI: Select Color Scheme"
			],
			[
				"reinde",
				"Indentation: Reindent Lines"
			],
			[
				"remoe",
				"Package Control: Remove Package"
			],
			[
				"pack",
				"Package Control: Install Package"
			],
			[
				"color sc",
				"UI: Select Color Scheme"
			],
			[
				"color sch",
				"UI: Select Color Scheme"
			],
			[
				"snipp",
				"Snippet: cp abhj c++"
			],
			[
				"material",
				"Material Theme: Configuration"
			],
			[
				"paackag",
				"Package Control: Install Package"
			],
			[
				"ind",
				"Indentation: Reindent Lines"
			]
		],
		"width": 0.0
	},
	"console":
	{
		"height": 224.0,
		"history":
		[
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/home/abj/Documents/cp",
		"/home/abj/Documents/cp/contest"
	],
	"file_history":
	[
		"/home/abj/Documents/7th sem notes/nlp/NLP_CS60075_A21_Assn1_18IE10001.ipynb",
		"/home/abj/Downloads/NLP_CS60075_A21_Assn1.ipynb",
		"/home/abj/Documents/cp/contest/a.cpp",
		"/home/abj/Documents/cp/contest/input.txt",
		"/home/abj/Documents/cp/contest/b.cpp",
		"/home/abj/Documents/cp/contest/c.cpp",
		"/home/abj/Documents/cp/contest/d.cpp",
		"/home/abj/Documents/cp/contest/e.cpp",
		"/home/abj/Documents/cp/contest/f.cpp",
		"/home/abj/Documents/cp/.gitignore",
		"/home/abj/Documents/cp/contest/.gitignore",
		"/home/abj/.config/sublime-text-3/Packages/User/cp.sublime-build",
		"/home/abj/Documents/cp/contest/notes.todo",
		"/home/abj/Documents/cp/contest/bits/stdc++.h",
		"/home/abj/Documents/cp/contest/output.txt",
		"/home/abj/Downloads/consistency_chapter_2_validation_input (1).txt",
		"/home/abj/Documents/cp/contest/saved_files/facebook_hcup_2021_qroundB.cpp",
		"/home/abj/Documents/cp/contest/saved_files/facebook_hcup_2021_qroundC1.cpp",
		"/home/abj/Documents/learning_react/abj-blogs/src/components/RecentBlogsHeading.js",
		"/home/abj/Documents/learning_mern/client/src/App.js",
		"/home/abj/Documents/learning_mern/server/index.js",
		"/home/abj/Documents/learning_mern/client/src/index.js",
		"/home/abj/Documents/cp/contest/saved_files/codeforces1557C.cpp",
		"/home/abj/Documents/cp/contest/saved_files/codeforces1559C.cpp",
		"/home/abj/Downloads/c.cpp",
		"/home/abj/Documents/cp/contest/3",
		"/home/abj/Documents/cp/contest/g.cpp",
		"/home/abj/Documents/cp/contest/h.cpp",
		"/home/abj/Documents/cp/contest/atcoder/dsu.hpp",
		"/home/abj/Documents/cp/cp.sublime-project",
		"/home/abj/Documents/cp/README.md",
		"/home/abj/Documents/cp/contest/atcoder/convolution.hpp",
		"/home/abj/Documents/cp/contest/atcoder/dsu",
		"/home/abj/Documents/cp/contest/a.out",
		"/home/abj/Documents/cp/contest/runfile",
		"/home/abj/Documents/cp/contest/saved_files/abc204_a.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc214_d.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc214_e.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc214_c.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc214_b.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc214_a.cpp",
		"/home/abj/Documents/cp/templates/segment_tree.cpp",
		"/home/abj/Documents/cp/.ycm_extra_conf.py",
		"/home/abj/Documents/cp/contest/saved_files/leetcode/887.cpp",
		"/home/abj/Documents/cp/contest/saved_files/leetcode/1927.pdf",
		"/home/abj/Documents/cp/contest/saved_files/leetcode/805.cpp",
		"/home/abj/Documents/cp/contest/saved_files/leetcode/403.cpp",
		"/home/abj/Documents/cp/templates/template.cpp",
		"/home/abj/Documents/cp/contest/note.todo",
		"/home/abj/Documents/cp/contest/saved_files/codeforces1494D.cpp",
		"/home/abj/Documents/cp/templates/atcoder documentation/README.md",
		"/home/abj/.config/sublime-text-3/Packages/User/sparse_table_struct.sublime-snippet",
		"/home/abj/.config/sublime-text-3/Packages/User/sparse_table.sublime-snippet",
		"/home/abj/Documents/cp/contest/saved_files/codeforces1549D.cpp",
		"/home/abj/Documents/cp/templates/atcoder documentation/document_en/math.md",
		"/home/abj/.config/sublime-text-3/Packages/User/cp.sublime-snippet",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc209b.cpp",
		"/home/abj/.vimrc",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc209c.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc209d.cpp",
		"/home/abj/.config/sublime-text-3/Packages/User/add_date.py",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc209a.cpp",
		"/home/abj/.vim/templates/templates/mult.cpp",
		"/home/abj/.vim/templates/templates/sing.cpp",
		"/home/abj/.config/sublime-text-3/Packages/User/Preferences.sublime-settings",
		"/home/abj/Documents/cp/contest/sa.cpp",
		"/home/abj/.config/sublime-text-3/Packages/User/lca.sublime-snippet",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc212d.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc212c.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc212b.cpp",
		"/home/abj/Documents/cp/contest/saved_files/atcoder_abc212a.cpp",
		"/home/abj/Documents/cp/contest/saved_files/codeforces1030D.cpp",
		"/home/abj/Documents/cp/contest/rules_of_life.txt",
		"/home/abj/Documents/cp/contest/saved_files/codechefMODEQUAL.cpp",
		"/home/abj/Documents/cp/contest/codeforces1554B.cpp",
		"/home/abj/Documents/cp/contest/saved_files/codeforces1554D.cpp",
		"/home/abj/Documents/cp/contest/saved_files/codeforces1554C.cpp",
		"/home/abj/Documents/cp/contest/saved_files/codeforces1554A.cpp",
		"/home/abj/Documents/cp/contest/saved_files/README.md",
		"/home/abj/Documents/cp/contest/saved_files/codeforces301A.cpp",
		"/home/abj/Documents/cp/contest/saved_files/codeforces7C.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces652D.cpp",
		"/home/abj/Documents/cp/saved_files/abc204_a.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1225D.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces612D.cpp",
		"/home/abj/Documents/cp/saved_files/atcoder_arc124b.cpp",
		"/home/abj/Documents/cp/contest/.a.cpp.swp",
		"/home/abj/Documents/cp/saved_files/atcoder_arc124a.cpp",
		"/home/abj/Documents/cp/saved_files/atcoder_abc211e.cpp",
		"/home/abj/Documents/cp/saved_files/atcoder_abc211b.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1538E.cpp",
		"/home/abj/Documents/cp/templates/atcoder documentation/test/unittest/string_test.cpp",
		"/home/abj/Documents/BTP/Plane Drawing/Triangle Voxelization/a.obj",
		"/home/abj/Documents/cp/saved_files/codeforces1552C.cpp",
		"/tmp/mozilla_abj0/codeforces1547G.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1547G.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1552D.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1553E.cpp",
		"/home/abj/.config/sublime-text-3/Packages/User/bfs.sublime-snippet",
		"/home/abj/Documents/cp/saved_files/codeforces1547F.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1538F.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1551E.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1551D1.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1551B2.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1551D1.pdf",
		"/home/abj/Documents/cp/saved_files/codeforces1551C.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1551B1.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1551A.cpp",
		"/home/abj/.config/sublime-text-3/Packages/User/ulrich_nielsen.sublime-snippet",
		"/home/abj/Documents/cp/saved_files/leetcode/1911.cpp",
		"/home/abj/Documents/cp/saved_files/leetcode/1910.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1536E.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1537E2.cpp",
		"/home/abj/Documents/cp/untitled",
		"/home/abj/Documents/cp/saved_files/leetcode/1915.cpp",
		"/home/abj/Documents/cp/saved_files/abc204_b.cpp",
		"/home/abj/Documents/cp/saved_files/abc204_e.cpp",
		"/home/abj/Documents/cp/contest/bits/stdc++.h.gch",
		"/home/abj/.config/sublime-text-3/Packages/User/Function Name Display.sublime-settings",
		"/home/abj/.config/sublime-text-3/Packages/User/ClangAutoComplete.sublime-settings",
		"/home/abj/.config/sublime-text-3/Packages/ClangAutoComplete/ClangAutoComplete.sublime-settings",
		"/home/abj/.config/sublime-text-3/Packages/Function Name Display/Function Name Display.sublime-settings",
		"/home/abj/Documents/cp/saved_files/leetcode/1928.cpp",
		"/home/abj/Documents/cp/contest/1928.cpp",
		"/home/abj/Documents/cp/saved_files/leetcode/1921.cpp",
		"/home/abj/Documents/cp/saved_files/atcoder_abc210d.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1530E.cpp",
		"/home/abj/Documents/cp/saved_files/codeforces1530D.cpp"
	],
	"find":
	{
		"height": 24.0
	},
	"find_in_files":
	{
		"height": 652.0,
		"where_history":
		[
			"/home/abj/Documents/cp/templates,<project filters>",
			"/home/abj/Documents/cp/contest,<project filters>",
			""
		]
	},
	"find_state":
	{
		"case_sensitive": true,
		"find_history":
		[
			"c",
			"target",
			"&&",
			"&& ",
			"INT_MAX",
			"k",
			"ans",
			"p",
			"ans",
			"mp",
			"an",
			"c",
			"cn",
			"}]",
			" {",
			"fl",
			"vis",
			"vi",
			"INT_MAX / 2",
			" ;",
			"d",
			"visited",
			"i",
			"fg",
			"adj",
			"d",
			"mp",
			"INT_MIN",
			"mx",
			"ar",
			"ll",
			"a",
			"pos",
			"q",
			"fs",
			"row",
			"col",
			"x",
			"y",
			"x",
			"push_back",
			"prevCount",
			"1010",
			"mat",
			"for (int i = 1; i <= n; i++) {\n\t\tfor (int j = 1; j <= m; j++) \n\t\t\tcout << a[i][j];\n\t\tcout << \"\\n\";\n\t}",
			"mat",
			"mat[i][j] >= 'A' and mat[i][j] <= 'Z'",
			"countAnswer",
			"x",
			"arr",
			"ch",
			"p",
			"freq",
			"cnt",
			"v",
			"v2",
			"mx",
			"m1",
			"v1",
			"vv",
			"pair<int, char>",
			"SS",
			"SSS",
			"SS",
			"],\n",
			"SS",
			"ans",
			"v2",
			"v1",
			"m1",
			"v1",
			"m1",
			"mx",
			"p",
			"vv",
			"pair<int, char>",
			"v",
			"idx",
			"val",
			"size",
			"temp",
			"nequal",
			"equal",
			"nequal",
			"pb",
			"str",
			"flag",
			"i",
			"p",
			"str",
			"ll",
			"arr",
			"x",
			"cnt",
			"i",
			"sp",
			"sieve",
			"p",
			"prime",
			"log",
			"lo[",
			"log",
			"R",
			"L",
			"u",
			"bitch",
			"z",
			"q",
			"cnt",
			"big",
			"cnt",
			"z",
			"big",
			"subtree",
			"bfs.distance",
			"#include \"bits/stdc++.h\"\n#define int          long long int\n#define mp           make_pair\n#define pb           emplace_back\n#define F            first\n#define S            second\nusing vi       =     std::vector<int>;\nusing vvi      =     std::vector<vi>;\nusing pii      =     std::pair<int, int>;\nusing vpii     =     std::vector<pii>;\nusing vvpii    =     std::vector<vpii>;\nusing namespace std;\n#define rep(i, a, b) for (int i = a; i <= b; i++)\nconst int inf  =     1e18 + 10;\nconst int N    =     5e3 + 10;\nint n, m, k, x, y, cnt;\nconst int M    =     998244353;\nint add(int a, int b) {\n\ta += b;\n\treturn (a >= M ? a - M : a);\n}\nint mul(int a, int b) {\n\treturn (a * b) % M;\n}\nint sub(int a, int b) {\n\treturn (a - b + M) % M;\n}\nint powM(int b, int p) {\n\tint r = 1;\n\tfor (; p; b = mul(b, b), p >>= 1)\n\t\tif (p & 1)\n\t\t\tr = mul(r, b);\n\treturn r;\n}\nvvi p(vvi a, vvi b) {\n\tvvi x(a.size(), vi(b[0].size()));\n\trep(i, 0, a.size() - 1)\n\trep(j, 0, b[0].size() - 1)\n\trep(k, 0, a[0].size() - 1)\n\tx[i][j] = add(x[i][j], mul(a[i][k], b[k][j]));\n\treturn x;\n}\nint invM(int x) {\n\treturn powM(x, M - 2);\n}\nint f[N], b[N];\nvoid binomialCoeff() {\n\tf[0] = 1;\n\tfor (int i = 1; i <= N - 1; i++)\n\t\tf[i] = mul(f[i - 1], i);\n\tb[N - 1] = powM(f[N - 1], M - 2);\n\tfor (int i = N - 2; i >= 0; i--)\n\t\tb[i] = mul(b[i + 1], i + 1);\n}\nint C(int n, int r) {\n\treturn (r > n ? 0LL : mul(f[n], mul(b[r], b[n - r])));\n}\nvvi a;\n//this return pow(A,x) where A is matrix\nvvi matrix_power_final(vvi A, int x) {\n\tvvi result(n, vi(n, 0));\n\trep(i, 0, n - 1)result[i][i] = 1;\n\twhile (x) {\n\t\tif (x & 1)result = p(result , A);\n\t\tA = p(A , A);\n\t\tx = x / 2;\n\t}\n\treturn result;\n}\nstd::vector<std::vector<int>> multiply(std::vector<std::vector<int>> a,\n                                       std::vector<std::vector<int>> b)\n{\n\t// Creating an auxiliary matrix to store elements\n\t// of the multiplication matrix\n\tint __v = a.size();\n\tstd::vector<std::vector<int>> MU(__v, std::vector<int>(__v));\n\n\tfor (int i = 0; i < __v; ++i)\n\t{\n\t\tfor (int j = 0; j < __v; ++j)\n\t\t{\n\t\t\tMU[i][j] = 0;\n\t\t\tfor (int l = 0; l < __v; ++l)\n\t\t\t\tMU[i][j] = add(MU[i][j], mul(a[i][l] , b[l][j]));\n\t\t}\n\t}\n\n\treturn MU;\n}\n\n// Function to compute adj raise to power k.\nint power(std::vector<std::vector<int>> adj, int u, int v, int k)\n{\n\tint __v = adj.size();\n\tstd::vector<std::vector<int>> res(__v, std::vector<int>(__v));\n\n\tfor (int i = 0; i < __v; ++i)\n\t\tres[i][i] = 1;\n\n\twhile (k > 0)\n\t{\n\t\tif (k % 2 == 1)\n\t\t\tres = multiply(res, adj);\n\t\tadj = multiply(adj, adj);\n\t\tk /= 2;\n\t}\n\t// number of paths from u to v with k edges\n\treturn res[u][v];\n}\n\n// function to find total number of paths\n// with k edges.\nint numberOfPathsDivideConquer(std::vector<std::vector<int>> adj, int u, int v, int k)\n{\n\treturn power(adj, u, v, k);\n}\nvoid solve() {\n\tcin >> n >> m >> k;\n\tmap<int, int>ma;\n\tma[0] = 0;\n\tvpii ed;\n\tint cnt = 1;\n\tfor (int i = 1; i <= m; i++) {\n\t\tcin >> x >> y;\n\t\tx--, y--;\n\t\tif (ma.count(x) == 0)\n\t\t\tma[x] = cnt++;\n\t\tif (ma.count(y) == 0)\n\t\t\tma[y] = cnt++;\n\n\t\ted.pb(x, y);\n\t}\n\ta = vvi(cnt + 1, vi(cnt + 1));\n\tfor (int i = 0; i < cnt; i++) {\n\t\tfor (int j = 0; j < cnt; j++) if (i != j) {\n\t\t\t\ta[i][j] = 1;\n\t\t\t}\n\t}\n\tfor (auto [x, y] : ed) {\n\t\ta[ma[x]][ma[y]] = a[ma[y]][ma[x]] = 0;\n\t}\n\tn = cnt + 1;\n\ta = matrix_power_final(a, k);\n\tcout << a[0][0];\n}\nint32_t main() {\n\tios_base::sync_with_stdio(false); cin.tie(0);\n\tsolve(); return 0;\n\treturn 0;\n}",
			"ma.size()",
			"m",
			"n",
			"x",
			"mul",
			"d",
			"k",
			"s",
			"pos",
			"x",
			"y",
			"ans"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
		],
		"reverse": false,
		"scrollbar_highlights": true,
		"show_context": true,
		"use_buffer2": true,
		"use_gitignore": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"sheets":
			[
				{
					"buffer": 0,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1,
						"regions":
						{
						},
						"selection":
						[
							[
								1,
								1
							]
						],
						"settings":
						{
							"auto_name": "",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"stack_multiselect": false,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "/home/abj/Downloads/NLP_CS60075_A21_Assn1 (1).ipynb",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 22961,
						"regions":
						{
						},
						"selection":
						[
							[
								321,
								321
							]
						],
						"settings":
						{
							"syntax": "Packages/JSON/JSON.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 2429.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"stack_multiselect": false,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "contest/d.cpp",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"stack_multiselect": false,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "contest/c.cpp",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 645,
						"regions":
						{
						},
						"selection":
						[
							[
								308,
								308
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"stack_multiselect": false,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "/home/abj/Documents/7th sem notes/nlp/NLPAssignments/NLP_CS60075_A21_Assn1_18IE10001.ipynb",
					"selected": true,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 34457,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/JSON/JSON.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 3555.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"stack_multiselect": false,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "/home/abj/Documents/text_docs/time.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"stack_multiselect": false,
					"type": "text"
				}
			]
		},
		{
			"sheets":
			[
			]
		},
		{
			"sheets":
			[
			]
		}
	],
	"incremental_find":
	{
		"height": 24.0
	},
	"input":
	{
		"height": 220.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				2
			],
			[
				1,
				0,
				2,
				1
			],
			[
				1,
				1,
				2,
				2
			]
		],
		"cols":
		[
			0.0,
			0.745870827651,
			1.0
		],
		"rows":
		[
			0.0,
			0.5,
			1.0
		]
	},
	"menu_visible": true,
	"output.SublimeLinter":
	{
		"height": 90.0
	},
	"output.astyle_error_message":
	{
		"height": 0.0
	},
	"output.clangautocomplete":
	{
		"height": 120.0
	},
	"output.doc":
	{
		"height": 0.0
	},
	"output.exec":
	{
		"height": 253.0
	},
	"output.find_results":
	{
		"height": 0.0
	},
	"output.git":
	{
		"height": 108.0
	},
	"pinned_build_system": "Packages/User/cp.sublime-build",
	"project": "cp.sublime-project",
	"replace":
	{
		"height": 44.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"",
				"codeforces1515e.cpp"
			],
			[
				"d.cpp",
				"d.cpp"
			],
			[
				"e.cpp",
				"e.cpp"
			],
			[
				"b.c",
				"b.cpp"
			],
			[
				"a.cpp",
				"a.cpp"
			],
			[
				"b.cpp",
				"b.cpp"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 500.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"",
				"~/Downloads/PortfolioAbJ-main/portfolio-heroku.sublime-project"
			]
		],
		"width": 380.0
	},
	"select_symbol":
	{
		"height": 55.0,
		"last_filter": "spf",
		"selected_items":
		[
			[
				"spf",
				"simplify_fraction"
			],
			[
				"powM",
				"powM"
			],
			[
				"",
				"powM"
			],
			[
				"fact",
				"calc_fact"
			]
		],
		"width": 521.0
	},
	"selected_group": 0,
	"settings":
	{
		"last_automatic_layout":
		[
			[
				0,
				0,
				1,
				2
			],
			[
				1,
				0,
				2,
				1
			],
			[
				1,
				1,
				2,
				2
			]
		]
	},
	"show_minimap": false,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 219.0,
	"status_bar_visible": true,
	"template_settings":
	{
		"max_columns": 2
	}
}
